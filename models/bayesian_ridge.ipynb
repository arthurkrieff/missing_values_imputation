{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "from functions.metrics import nrmse_adjusted\n",
    "from functions.utils import to_NAN, put_historical_nans\n",
    "from functions.import_data import import_datasets\n",
    "\n",
    "from blob_credentials import facts_sas_token, facts_container, workspace_sas_token, workspace_container\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import linear_model\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_df(df):\n",
    "    \"\"\"\n",
    "    Function that shuffle the columns of df\n",
    "    Input:\n",
    "        - df: pd.DataFrame\n",
    "    Output:\n",
    "        - pd.DataFrame shuffled\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    random.seed(0)\n",
    "    random.shuffle(cols)\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_df(df):\n",
    "    \"\"\"\n",
    "    Function that orders columns based on their value\n",
    "    Input:\n",
    "        - df: pd.DataFrame, where columns are originally string representing Integers\n",
    "    Output:\n",
    "        - pd.DataFrame where columns are ordered in an ascending order\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    cols_int = [int(i) for i in cols]\n",
    "    cols_int.sort()\n",
    "    cols_str = [str(i) for i in cols_int]\n",
    "    return df[cols_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_beg_end(df, subset_size, i):\n",
    "    \"\"\"\n",
    "    Function that computes the indices of the columns that should be taken into account\n",
    "    Input:\n",
    "        - df: pd.DataFrame\n",
    "        - subset_size: integer\n",
    "        - i: integer, step value\n",
    "    Output:\n",
    "        - int, int: indices of first and last column\n",
    "    \"\"\"\n",
    "    total_subsets = df.shape[1]//subset_size\n",
    "    if i == (total_subsets-1): # Last subset\n",
    "        beg = i*subset_size\n",
    "        end = df.shape[1]\n",
    "    else:\n",
    "        beg = i*subset_size\n",
    "        end = (i+1)*subset_size\n",
    "    return beg, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_subsets(df, subset_size):\n",
    "    \"\"\"\n",
    "    Function that computes the number of subsets for a given dataframe\n",
    "    Input:\n",
    "        - df: pd.DataFrame\n",
    "        - subset_size: int\n",
    "    Output:\n",
    "        - int, representing the number of subsets the df should be divided into\n",
    "    \"\"\"\n",
    "    if df.shape[1]//subset_size == 0:\n",
    "        total_subsets = 1\n",
    "    else:\n",
    "        total_subsets = df.shape[1]//subset_size\n",
    "    return total_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_bayesianRidge(df, subset_size=100):\n",
    "    \"\"\"\n",
    "    Function that imputes BayesianRidge based on data subsets of subset_size\n",
    "    Input:\n",
    "        - df: pd.DataFrame, dataframe that should be imputed\n",
    "        - subset_size: integer\n",
    "    Output:\n",
    "        - pd.DataFrame imputed\n",
    "    \"\"\"\n",
    "    dfs_imputed = {}\n",
    "    df = shuffle_df(df)\n",
    "    total_subsets = compute_total_subsets(df, subset_size)\n",
    "    for i in tqdm(range(total_subsets)): \n",
    "        # Create Subset\n",
    "        beg, end = compute_beg_end(df, subset_size, i)\n",
    "        df_sub = df.iloc[:, beg:end]\n",
    "        cols = df_sub.columns\n",
    "        \n",
    "        # Run Imputation\n",
    "        clf = linear_model.BayesianRidge()\n",
    "        imputer = IterativeImputer(estimator=clf, n_nearest_features=None, \n",
    "                               imputation_order='ascending', random_state=0, verbose=2)\n",
    "        df_imputed = imputer.fit_transform(df_sub)\n",
    "        \n",
    "        # Clean DF\n",
    "        df_imputed = pd.DataFrame(df_imputed)\n",
    "        df_imputed.columns = pd.Index(cols)\n",
    "        dfs_imputed[i] = df_imputed\n",
    "    \n",
    "    full_imputed = pd.concat(dfs_imputed.values(), axis=1)\n",
    "    full_imputed = order_df(full_imputed)\n",
    "    return full_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = \"marc-samvath-philippe.vigneron\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(f\"Test-{myname}\") \\\n",
    "    .config(\"spark.executor.instance\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\",\"512m\") \\\n",
    "    .config('spark.jars.packages',\"org.apache.hadoop:hadoop-azure:3.1.1\") \\\n",
    "    .config(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\") \\\n",
    "    .config(\"fs.wasbs.impl\",\"org.apache.hadoop.fs.azure.NativeAzureFileSystem\") \\\n",
    "    .config(f\"fs.azure.sas.{facts_container}.hecdf.blob.core.windows.net\", facts_sas_token) \\\n",
    "    .config(f\"fs.azure.sas.{workspace_container}.hecdf.blob.core.windows.net\", workspace_sas_token) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "generated_data_gbm = spark.read.parquet(f'wasbs://{workspace_container}@hecdf.blob.core.windows.net/{myname}/generated_data_gbm.parquet').toPandas()\n",
    "generated_data_kde = spark.read.parquet(f'wasbs://{workspace_container}@hecdf.blob.core.windows.net/{myname}/generated_data_kde.parquet').toPandas()\n",
    "df_challenge = import_datasets()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting the values to nan\n",
    "dataset_challenge_gbm_nan = to_NAN(generated_data_gbm, df_challenge.drop(columns=[\"Date\"]))\n",
    "dataset_challenge_kde_nan = to_NAN(generated_data_kde, df_challenge.drop(columns=[\"Date\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "df_imputed_gbm = impute_bayesianRidge(dataset_challenge_gbm_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical NaNs\n",
    "df_imputed_gbm = put_historical_nans(df_imputed_gbm, dataset_challenge_gbm_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gbm = nrmse_adjusted(generated_data_gbm.values, \n",
    "                             df_imputed_gbm.values,\n",
    "                             dataset_challenge_gbm_nan.values)\n",
    "\n",
    "mean_nrmse_gbm = np.nanmean(np.array(list(i[0] for i in results_gbm.values())))\n",
    "print(\"GBM iterative imputer NRMSE: %f\" % mean_nrmse_gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "df_imputed_kde = impute_bayesianRidge(dataset_challenge_kde_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Historical NaNs\n",
    "df_imputed_kde = put_historical_nans(df_imputed_kde, dataset_challenge_gbm_kde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_kde = nrmse_adjusted(generated_data_kde.values, \n",
    "                             df_imputed_kde.values,\n",
    "                             dataset_challenge_kde_nan.values)\n",
    "mean_nrmse_kde = np.nanmean(np.array(list(i[0] for i in results_kde.values())))\n",
    "print(\"kde iterative imputer NRMSE: %f\" % mean_nrmse_kde)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
